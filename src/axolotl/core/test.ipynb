{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f8b2e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import abc\n",
    "import importlib\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler, OneCycleLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler, SequentialSampler\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "from transformers.trainer_pt_utils import SequentialDistributedSampler\n",
    "from transformers.trainer_utils import has_length\n",
    "\n",
    "from axolotl.monkeypatch.relora import ReLoRACallback, ReLoRAScheduler\n",
    "from axolotl.utils.callbacks import (\n",
    "    EvalFirstStepCallback,\n",
    "    GPUStatsCallback,\n",
    "    SaveAxolotlConfigtoWandBCallback,\n",
    "    SaveBetterTransformerModelCallback,\n",
    "    bench_eval_callback_factory,\n",
    "    log_prediction_callback_factory,\n",
    ")\n",
    "from axolotl.utils.collators import DataCollatorForSeq2Seq\n",
    "from axolotl.utils.dataloader import MultipackDistributedDataloader, StreamingMultipackDistributedDataloaderNew\n",
    "from axolotl.utils.schedulers import get_cosine_schedule_with_quadratic_warmup\n",
    "from axolotl.utils.distributed import (\n",
    "    is_main_process,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import torch._dynamo  # pylint: disable=ungrouped-imports\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "LOG = logging.getLogger(\"axolotl.core.trainer_builder\")\n",
    "\n",
    "\n",
    "class CandidatePenaltyCrossEntropyCriterion():\n",
    "    \"\"\"Applies a (1-p(x_nt)) loss to each negative target ('candidate') x_nt.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.padding_idx = 0\n",
    "        self.IGNORE_TOKEN_ID = -100  # Copied from prompt_strategies\n",
    "\n",
    "    def forward(self, target, pred_logits):\n",
    "        shift_targets = target[..., 1:].contiguous()\n",
    "        shift_targets = shift_targets.view(-1)\n",
    "        shift_targets = shift_targets.masked_fill(shift_targets == self.IGNORE_TOKEN_ID, self.padding_idx)\n",
    "        mask_ignore = shift_targets != self.padding_idx\n",
    "        shift_targets = shift_targets[mask_ignore]\n",
    "        print(shift_targets.shape)\n",
    "\n",
    "        shift_logits = pred_logits[..., :-1, :].contiguous()\n",
    "        shift_lprobs = F.log_softmax(shift_logits, dim=-1)\n",
    "        shift_lprobs = shift_lprobs.view(-1, shift_lprobs.size(-1))\n",
    "        shift_lprobs = shift_lprobs[mask_ignore]\n",
    "        print(shift_lprobs.shape)\n",
    "\n",
    "        # # Sanity check that this is the same as primary_loss.\n",
    "        # sanity_mle_loss = F.nll_loss(\n",
    "        #     shift_lprobs,\n",
    "        #     shift_targets,\n",
    "        #     reduction='mean')\n",
    "\n",
    "        # -- unliklihood loss\n",
    "        # Maximize (1 - p(x_nt)) for negative target tokens x_nt (equivalently minimize -log(1-p(x_nt)))\n",
    "\n",
    "        # - form negative targets\n",
    "        with torch.no_grad():\n",
    "            # E.g. DABCC | D | EFFGD => {A,B,C} are negative targets.\n",
    "            # Make 'the triangle'.\n",
    "            # There's still a bug since we have packed batches: https://github.com/facebookresearch/unlikelihood_training/issues/11#issue-1630788451\n",
    "            #ctx_cands = shift_targets.unsqueeze(0).expand(shift_targets.size(0), shift_targets.size(0))\n",
    "            ctx_cands = shift_targets.unsqueeze(0).repeat(shift_targets.size(0), 1)\n",
    "            print(ctx_cands)\n",
    "            rows, cols = torch.triu_indices(shift_targets.size(0), shift_targets.size(0))\n",
    "            ctx_cands[rows, cols] = 0\n",
    "            print(ctx_cands)\n",
    "            # Don't include the target for that timestep as a negative target.\n",
    "            ctx_cands = ctx_cands.masked_fill(ctx_cands == shift_targets.unsqueeze(1), self.padding_idx)\n",
    "            print(ctx_cands)\n",
    "            negative_targets = torch.zeros_like(shift_lprobs).scatter_(1, ctx_cands, 1)\n",
    "\n",
    "        # - compute loss\n",
    "        one_minus_probs = torch.clamp((1.0 - shift_lprobs.exp()), min=1e-5)\n",
    "        unliklihood_loss = -torch.log(one_minus_probs)*negative_targets\n",
    "        unliklihood_loss = unliklihood_loss.sum(1).mean()\n",
    "        return unliklihood_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1354613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a a b b b pad pad pad\n",
    "inputs = torch.tensor([[1, 1, 2, 2, 2, -100, -100, -100]])\n",
    "pred_logits = torch.tensor([[[0.0, 0.0, 0.0],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                            [0.0, 0.9, 0.1],\n",
    "                           ]])\n",
    "\n",
    "loss_fn = CandidatePenaltyCrossEntropyCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f3bd5723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 3])\n",
      "tensor([[1, 2, 2, 2],\n",
      "        [1, 2, 2, 2],\n",
      "        [1, 2, 2, 2],\n",
      "        [1, 2, 2, 2]])\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 2, 0, 0],\n",
      "        [1, 2, 2, 0]])\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8673)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn.forward(inputs, pred_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axolotl",
   "language": "python",
   "name": "axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
